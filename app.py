# -*- coding: utf-8 -*-
"""ChurnAnalysis.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ijl2rMTIxH9Et7XVOlA7eQhjNZh2R27H
"""

import pandas as pd  # For handling data in tabular form
import missingno as msno  # For visualizing missing data patterns
import matplotlib.pyplot as plt  # For basic plotting
import seaborn as sns  # For advanced data visualization
import numpy as np  # For numerical operations
from sklearn import preprocessing  # For preprocessing data before modeling
from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets
from imblearn.over_sampling import SMOTE  # For handling imbalanced datasets
from sklearn.ensemble import RandomForestClassifier  # For building a random forest classifier
from sklearn.metrics import accuracy_score  # For evaluating model performance
import plotly.express as px
import plotly.graph_objs as go

from google.colab import files
uploaded=files.upload()

df=pd.read_csv("Input.csv")

print("Number of Columns in dataset",df.shape[1])
print("---------------------------------------")
print("Number of Rows in  dataset",df.shape[0])

df.head()

df.tail()

df.info()

df.dtypes

df.columns

df.nunique()

df["Churn"].value_counts()

msno.matrix(df);

df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')
df.isnull().sum()

plt.figure(figsize=(6, 6))
labels =["Churn: Yes","Churn:No"]
values = [1869,5163]
labels_gender = ["F","M","F","M"]
sizes_gender = [939,930 , 2544,2619]
colors = ['#ff6666', '#66b3ff']
colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']
explode = (0.3,0.3)
explode_gender = (0.1,0.1,0.1,0.1)
textprops = {"fontsize":15}
#Plot
plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
9fig.gca().add_artist(centre_circle)

plt.title('Churn Distribution w.r.t Gender: Male(M), Female(F)', fontsize=15, y=1.1)

# show plot

plt.axis('equal')
plt.tight_layout()
plt.show()

fig = px.histogram(df, x="Churn", color="Contract", barmode="group", title="<b>Customer contract distribution<b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

labels = df['PaymentMethod'].unique()
values = df['PaymentMethod'].value_counts()

fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_layout(title_text="<b>Payment Method Distribution</b>")
fig.show()

fig = px.histogram(df, x="Churn", color="PaymentMethod", title="<b>Customer Payment Method distribution w.r.t. Churn</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="Dependents", barmode="group", title="<b>Dependents distribution</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="Partner", barmode="group", title="<b>Chrun distribution w.r.t. Partners</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": '#00CC96', "No": '#B6E880'}
fig = px.histogram(df, x="Churn", color="SeniorCitizen", title="<b>Chrun distribution w.r.t. Senior Citizen</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="OnlineSecurity", barmode="group", title="<b>Churn w.r.t Online Security</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="PaperlessBilling",  title="<b>Chrun distribution w.r.t. Paperless Billing</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

fig = px.histogram(df, x="Churn", color="TechSupport",barmode="group",  title="<b>Chrun distribution w.r.t. TechSupport</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": '#00CC96', "No": '#B6E880'}
fig = px.histogram(df, x="Churn", color="PhoneService", title="<b>Chrun distribution w.r.t. Phone Service</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

sns.boxplot(x='Churn', y='MonthlyCharges', data=df)

plt.figure(figsize=(15, 10))

corr = df.apply(lambda x: pd.factorize(x)[0]).corr()

mask = np.triu(np.ones_like(corr, dtype=bool))

ax = sns.heatmap(corr, mask=mask, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=.2, cmap='coolwarm', vmin=-1, vmax=1)

df['TotalCharges'] = df['TotalCharges'].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()
cat_features = df.drop(['customerID','TotalCharges','MonthlyCharges','SeniorCitizen','tenure'],axis=1)

cat_features.head()

le = preprocessing.LabelEncoder()
df_cat = cat_features.apply(le.fit_transform)
df_cat.head()

num_features = df[['customerID','TotalCharges','MonthlyCharges','SeniorCitizen','tenure']]
finaldf = pd.merge(num_features, df_cat, left_index=True, right_index=True)

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Select categorical features and drop non-numeric columns
cat_features = df.drop(['customerID', 'Churn'], axis=1)
cat_feature_names = cat_features.columns.tolist()

# Apply LabelEncoder to categorical features
le = preprocessing.LabelEncoder()
df[cat_feature_names] = cat_features.apply(le.fit_transform)

# Select numerical features
num_features = df[['TotalCharges', 'MonthlyCharges', 'SeniorCitizen', 'tenure']]

# Merge numerical and encoded categorical features
finaldf = pd.concat([num_features, df[cat_feature_names], df['Churn']], axis=1).dropna()

# Split into features (X) and target (y)
X = finaldf.drop(['Churn'], axis=1)
y = finaldf['Churn']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Apply SMOTE for oversampling
oversample = SMOTE(k_neighbors=5)
X_smote, y_smote = oversample.fit_resample(X_train, y_train)

# Update training sets
X_train, y_train = X_smote, y_smote

rf = RandomForestClassifier(random_state=46)
rf.fit(X_train,y_train)

preds = rf.predict(X_test)
print(accuracy_score(preds,y_test))

feature_importances = rf.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.show()

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=46)
dt.fit(X_train, y_train)

preds = dt.predict(X_test)
print(accuracy_score(preds, y_test))

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=46)
lr.fit(X_train, y_train)

preds = lr.predict(X_test)
print(accuracy_score(preds, y_test))

le = preprocessing.LabelEncoder()
df[cat_feature_names] = cat_features.apply(le.fit_transform)

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV

# Assuming you have already loaded the dataset and performed the necessary data cleaning steps
# ...

# Convert 'TotalCharges' to numeric with error='coerce'
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Select categorical features and drop non-numeric columns
cat_features = df.drop(['customerID', 'Churn'], axis=1)
cat_feature_names = cat_features.columns.tolist()

# Apply LabelEncoder to categorical features
le = preprocessing.LabelEncoder()
df[cat_feature_names] = cat_features.apply(le.fit_transform)

# Encode the target variable 'Churn'
df['Churn'] = le.fit_transform(df['Churn'])

# Select numerical features
num_features = df[['TotalCharges', 'MonthlyCharges', 'SeniorCitizen', 'tenure']]

# Merge numerical and encoded categorical features
finaldf = pd.concat([num_features, df[cat_feature_names], df['Churn']], axis=1).dropna()

# Split into features (X) and target (y)
X = finaldf.drop(['Churn'], axis=1)
y = finaldf['Churn']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Apply SMOTE for oversampling
oversample = SMOTE(k_neighbors=5)
X_smote, y_smote = oversample.fit_resample(X_train, y_train)

# Update training sets
X_train, y_train = X_smote, y_smote

# Feature Scaling (optional but often beneficial for tree-based models)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# XGBoost model
xgb = XGBClassifier(random_state=46)

# Hyperparameter Tuning (optional but can improve model performance)
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200]
}

grid_search = GridSearchCV(XGBClassifier(random_state=46), param_grid, cv=3)
grid_search.fit(X_train_scaled, y_train)

best_xgb = grid_search.best_estimator_
preds_xgb = best_xgb.predict(X_test_scaled)

# Evaluation Metrics
print("Accuracy:", accuracy_score(preds_xgb, y_test))
print("Classification Report:\n", classification_report(y_test, preds_xgb))
print("AUC-ROC:", roc_auc_score(y_test, preds_xgb))
print("Best parameters:", grid_search.best_params_)

## for predicting the churns
from google.colab import files
uploaded=files.upload()
input_data = pd.read_csv('Book.csv')
input_data['TotalCharges'] = input_data['TotalCharges'].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()
# Add any additional preprocessing steps if needed

# Encode categorical columns
le = preprocessing.LabelEncoder()
input_cat_features = input_data[['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',
                                  'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
                                  'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']]

input_cat_features_encoded = input_cat_features.apply(le.fit_transform)

# Include 'customerID' in numerical features
input_num_features = input_data[['customerID', 'SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']]

# Merge numerical and categorical features
input_finaldf = pd.merge(input_num_features, input_cat_features_encoded, left_index=True, right_index=True)

# Drop 'customerID' column (assuming it's not used for prediction)
input_finaldf = input_finaldf.drop(['customerID'], axis=1)

# Load the trained model
rf = RandomForestClassifier(random_state=46)

# Assuming X_train and y_train are used for training the model
# If you have the entire dataset for training, you may need to adjust this accordingly
X_train, y_train = X_smote, y_smote
rf.fit(X_train, y_train)

# Ensure that the columns in input_finaldf are in the same order as X_train
input_finaldf = input_finaldf[X_train.columns]

# Make predictions on the input data
input_preds = rf.predict(input_finaldf)

# Display predictions
print(input_preds)